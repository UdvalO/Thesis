dL_dh_from_next <- matrix(0, nrow = hidden_dim, ncol = 1)
for (t in T_seq_len:1) {
h_t <- H[[t]]
x_t <- X[[t]]
h_prev_for_t <- if (t == 1) matrix(0, nrow = hidden_dim, ncol = 1) else H[[t-1]]
dL_dh_t_total <- dL_dh_from_next
if (t == T_seq_len) {
dL_dh_t_total <- dL_dh_t_total + (t(W_hy) %*% delta_o)
}
net_h <- net_inputs[[t]]
tanh_derivative <- 1 - (tanh(net_h))^2
delta_net_h <- dL_dh_t_total * tanh_derivative
dW_xh <- dW_xh + delta_net_h %*% t(x_t)
dW_hh <- dW_hh + delta_net_h %*% t(h_prev_for_t)
db_h <- db_h + delta_net_h
dL_dh_from_next <- t(W_hh) %*% delta_net_h
}
return(list(dW_xh = dW_xh, dW_hh = dW_hh, dW_hy = dW_hy, db_h = db_h, db_y = db_y))
}
rnn_update_weights <- function(weights, grads, learning_rate) {
weights$W_xh <- weights$W_xh - learning_rate * grads$dW_xh
weights$W_hh <- weights$W_hh - learning_rate * grads$dW_hh
weights$W_hy <- weights$W_hy - learning_rate * grads$dW_hy
weights$b_h <- weights$b_h - learning_rate * grads$db_h
weights$b_y <- weights$b_y - learning_rate * grads$db_y
return(weights)
}
rnn.cost.derivative.batch <- function(flat_weights, y, X, nn, hid_n, out_n, input_dim, h_initial_overall = NULL) {
# Unpack flat weights into matrices/vectors
weights <- rnn.weights.unpack(flat_weights, input_dim, hid_n, out_n)
num_samples <- nrow(X)
# Initialize total gradients as zero matrices/vectors
total_dW_xh <- matrix(0, nrow = hid_n, ncol = input_dim)
total_dW_hh <- matrix(0, nrow = hid_n, ncol = hid_n)
total_dW_hy <- matrix(0, nrow = out_n, ncol = hid_n)
total_db_h <- matrix(0, nrow = hid_n, ncol = 1)
total_db_y <- matrix(0, nrow = out_n, ncol = 1)
for (i in 1:num_samples) {
x_seq <- X[i, ]
y_true_val <- y[i]
# Forward pass for sample i
forward_out <- rnn_forward(x_seq, weights, h_prev = h_initial_overall)
# Backpropagation for sample i
grads <- rnn_bptt(y_true_val, forward_out, weights)
# Accumulate gradients
total_dW_xh <- total_dW_xh + grads$dW_xh
total_dW_hh <- total_dW_hh + grads$dW_hh
total_dW_hy <- total_dW_hy + grads$dW_hy
total_db_h <- total_db_h + grads$db_h
total_db_y <- total_db_y + grads$db_y
}
# Pack total gradients into flat vector (to match flat_weights shape)
packed_gradients <- rnn.weights.pack(list(
W_xh = total_dW_xh,
W_hh = total_dW_hh,
W_hy = total_dW_hy,
b_h = total_db_h,
b_y = total_db_y
))
return(packed_gradients)
}
rnn_predict_for_bgd_wrapper <- function(flat_weights, X, hid_n, out_n, input_dim, h_initial_overall = NULL) {
# Unpack weights vector into matrices
weights <- rnn.weights.unpack(flat_weights, input_dim, hid_n, out_n)
num_samples <- nrow(X)
y_preds <- numeric(num_samples)
# Loop over all sequences in X
for (i in 1:num_samples) {
# Run forward pass for each input sequence
forward_out <- rnn_forward(X[i, ], weights, h_prev = h_initial_overall)
y_preds[i] <- forward_out$y_hat
}
# Return list containing predictions vector
return(list(y_est = y_preds))
}
train_losses <- c()
val_losses <- c()
for (epoch in 1:100) {
# 1) Update weights by performing one BGD step here
trained_weights <- bgd(
weights = trained_weights,
y = y_train_scaled,
X = x_train,
hid_n = hidden_dim,
out_n = output_dim,
nn = rnn_predict_for_bgd_wrapper,
costderiv = rnn.cost.derivative.batch,
epochs = 1,
lr = 0.00001,
input_dim = input_dim
)
# 2) Predict on train set with current weights
train_preds_scaled <- rnn_predict_for_bgd_wrapper(
flat_weights = trained_weights,
X = x_train,
hid_n = 16,
out_n = 1,
input_dim = 1
)$y_est
# 3) Calculate training loss (MSE)
train_loss <- mean((train_preds_scaled - y_train_scaled)^2)
train_losses <- c(train_losses, train_loss)
# 4) Predict on validation/test set with current weights
val_preds_scaled <- rnn_predict_for_bgd_wrapper(
flat_weights = trained_weights,
X = x_test,
hid_n = 16,
out_n = 1,
input_dim = 1
)$y_est
# 5) Calculate validation loss (MSE)
val_loss <- mean((val_preds_scaled - y_test_scaled)^2)
val_losses <- c(val_losses, val_loss)
# Optionally print progress every some epochs
if (epoch %% 10 == 0) {
cat(sprintf("Epoch %d: Train Loss = %.4f | Val Loss = %.4f\n", epoch, train_loss, val_loss))
}
}
install.packages("quantmod")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(quantmod)
knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
message = FALSE
)
library(tidyverse)
library(quantmod)
getSymbols("SPY", src = "yahoo")
# Assign SPY data to a variable called `data`
data <- SPY
getSymbols("SPY", src = "yahoo")
# Assign SPY data to a variable called `data`
data <- SPY
View(data)
knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
message = FALSE
)
library(tidyverse)
library(quantmod)
library(xts)
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Step 3: Calculate daily percentage returns
spy_returns <- dailyReturn(spy_prices, type = "log") * 100  # log returns in percentage
# Optional: View the first few rows
head(spy_returns)
# 1. Plot returns
plot(log_returns, main = "Daily Log Returns of SPY")
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Step 3: Calculate daily percentage returns
spy_returns <- dailyReturn(spy_prices, type = "log") * 100  # log returns in percentage
# Optional: View the first few rows
head(spy_returns)
# 1. Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# 2. Test for stationarity (ADF Test)
adf.test(spy_returns, alternative = "stationary")
install.packages("tseries")
install.packages("PerformanceAnalytics")
install.packages("forecast")
library(tidyverse)
library(quantmod)
library(xts)
library(tseries)
library(PerformanceAnalytics)
library(forecast)
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Step 3: Calculate daily percentage returns
spy_returns <- dailyReturn(spy_prices, type = "log") * 100  # log returns in percentage
# Optional: View the first few rows
head(spy_returns)
# 1. Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# 2. Test for stationarity (ADF Test)
adf.test(spy_returns, alternative = "stationary")
# 3. Check for normality
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(log_returns)  # Q-Q plot
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Step 3: Calculate daily percentage returns
spy_returns <- dailyReturn(spy_prices, type = "log") * 100  # log returns in percentage
# Optional: View the first few rows
head(spy_returns)
# 1. Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# 2. Test for stationarity (ADF Test)
adf.test(spy_returns, alternative = "stationary")
# 3. Check for normality
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(spy_returns)  # Q-Q plot
shapiro.test(na.omit(log_returns))       # Shapiro-Wilk test
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Step 3: Calculate daily percentage returns
spy_returns <- dailyReturn(spy_prices, type = "log") * 100  # log returns in percentage
# Optional: View the first few rows
head(spy_returns)
# 1. Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# 2. Test for stationarity (ADF Test)
adf.test(spy_returns, alternative = "stationary")
# 3. Check for normality
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(spy_returns)  # Q-Q plot
shapiro.test(na.omit(spy_returns))       # Shapiro-Wilk test
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Step 3: Calculate daily percentage returns
spy_returns <- dailyReturn(spy_prices, type = "log") * 100  # log returns in percentage
# Optional: View the first few rows
head(spy_returns)
# 1. Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# 2. Test for stationarity (ADF Test)
adf.test(spy_returns, alternative = "stationary")
# 3. Check for normality
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(spy_returns)
# 4. Autocorrelation
acf(spy_returns, main = "ACF of Log Returns")
pacf(spy_returns)
# 5. Check for ARCH effects (Volatility clustering)
Box.test(spy_returns^2, lag = 12, type = "Ljung-Box")  # If p < 0.05, volatility clustering
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Step 3: Calculate daily percentage returns
spy_returns <- dailyReturn(spy_prices, type = "log") * 100  # log returns in percentage
# Optional: View the first few rows
head(spy_returns)
# 1. Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# 2. Test for stationarity (ADF Test)
adf.test(spy_returns, alternative = "stationary")
# 3. Check for normality
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(spy_returns)
# 4. Autocorrelation
acf(spy_returns, main = "ACF of Log Returns")
pacf(spy_returns)
# 5. Check for ARCH effects (Volatility clustering)
Box.test(spy_returns^2, lag = 12, type = "Ljung-Box")  # If p < 0.05, volatility clustering
pacf(spy_returns)
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(spy_returns)
adf.test(spy_returns, alternative = "stationary")
plot(spy_returns, main = "Daily Log Returns of SPY")
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Calculate daily percentage returns (log and in %)
spy_returns <- dailyReturn(spy_prices, type = "log") * 100
head(spy_returns)
# basic checking
# Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# Check for normality
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(spy_returns)
# Autocorrelation
acf(spy_returns, main = "ACF of Log Returns")
pacf(spy_returns)
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Calculate daily percentage returns (log and in %)
spy_returns <- dailyReturn(spy_prices, type = "log") * 100
head(spy_returns)
# basic checking
# 1. Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# 2. Test for stationarity (ADF Test)
adf.test(spy_returns, alternative = "stationary")
# 3. Check for normality
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(spy_returns)
# 4. Autocorrelation
acf(spy_returns, main = "ACF of Log Returns")
pacf(spy_returns)
# 5. Check for ARCH effects (Volatility clustering)
Box.test(spy_returns^2, lag = 12, type = "Ljung-Box")  # If p < 0.05, volatility clustering
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
spy_prices <- Ad(SPY)
# Calculate daily percentage returns (log and in %)
spy_returns <- dailyReturn(spy_prices, type = "log") * 100
head(spy_returns)
# basic checking
# Plot returns
plot(spy_returns, main = "Daily Log Returns of SPY")
# Check for normality
hist(spy_returns, breaks = 50, main = "Histogram of Log Returns")
qqnorm(spy_returns); qqline(spy_returns)
# Autocorrelation
acf(spy_returns, main = "ACF of Log Returns")
pacf(spy_returns)
# Delt() calculates the percent difference: (today - yesterday) / yesterday
returns <- Delt(Ad(SPY))
# Delt() calculates the percent difference: (today - yesterday) / yesterday
returns <- Delt(Ad(SPY))
# The first value will be NA, so we remove it
returns <- returns[-1, ]
# Rename the column for easier access
colnames(returns) <- "returns"
# Let's look at the first few rows of our data
cat("First 6 rows of daily returns for SPY:\n")
head(returns)
# downloading data
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
# 'Ad' extracts the Adjusted Close column
# Delt() calculates the percent difference: (today - yesterday) / yesterday
returns <- Delt(Ad(SPY))
# The first value will be NA, so we remove it
returns <- returns[-1, ]
# Rename the column for easier access
colnames(returns) <- "returns"
head(returns)
window_size <- 10
# The returns object is an 'xts' object from quantmod. We'll work with the core data.
returns_vec <- as.numeric(returns$returns)
make_state <- function(t_index) {
# The state is the vector of returns from (t - window_size + 1) to t.
# We need to check if the index is valid (i.e., we have enough past data)
if (t_index < window_size) {
stop("Not enough historical data to create a state vector.")
}
start_index <- t_index - window_size + 1
state_vector <- returns_vec[start_index:t_index]
return(state_vector)
}
window_size <- 10
# The returns object is an 'xts' object from quantmod. We'll work with the core data.
returns_vec <- as.numeric(returns$returns)
make_state <- function(t_index) {
# The state is the vector of returns from (t - window_size + 1) to t.
# We need to check if the index is valid (i.e., we have enough past data)
if (t_index < window_size) {
stop("Not enough historical data to create a state vector.")
}
start_index <- t_index - window_size + 1
state_vector <- returns_vec[start_index:t_index]
return(state_vector)
}
t_check <- 15
state_check <- make_state(t_check)
cat(sprintf("Check for t = %d:\n", t_check))
cat("The generated state vector is:\n")
print(state_check)
cat("\nThese are the returns from index 6 to 15:\n")
print(returns_vec[6:15])
library(tidyverse)
library(quantmod)
library(xts)
library(keras)
library(tensorflow)
# Define the model architecture
model <- keras_model_sequential(name = "Q-Network") %>%
# The hidden layer implicitly knows the input shape from the first call
layer_dense(units = 32, activation = "relu", input_shape = c(window_size),
name = "Hidden_Layer") %>%
# We use a 'linear' activation because Q-values are not probabilities;
layer_dense(units = 2, activation = "linear", name = "Q_Value_Output")
# Print a summary of our model
summary(model)
num_samples <- 256
num_returns <- length(returns_vec)
# We need to sample indices `t` where we can form a state AND check the next return.
# So, `t` must be between `window_size` and `num_returns - 1`.
sample_indices <- sample((window_size):(num_returns - 1),
size = num_samples,
replace = TRUE)
# --- Create the input matrix X ---
# Each row of X will be a state vector.
X_train <- t(sapply(sample_indices, make_state))
# --- Create the target matrix y ---
y_train <- matrix(0, nrow = num_samples, ncol = 2)
colnames(y_train) <- c("y_long", "y_flat")
for (i in 1:num_samples) {
t <- sample_indices[i]
# The rule is based on comparing the return at t+1 with the return at t.
ret_t <- returns_vec[t]
ret_t_plus_1 <- returns_vec[t + 1]
if (ret_t_plus_1 > ret_t) {
# If the next return is higher, the 'long' action was "correct".
y_train[i, "y_long"] <- 1
y_train[i, "y_flat"] <- 0
} else {
# Otherwise, the 'flat' action was "correct".
y_train[i, "y_long"] <- 0
y_train[i, "y_flat"] <- 1
}
}
cat("Dimensions of our training data:\n")
cat("X_train (inputs):", dim(X_train), "\n")
cat("y_train (targets):", dim(y_train), "\n\n")
cat("Example of one training pair:\n")
cat("State (X) for t =", sample_indices[1], "\n")
print(X_train[1,])
cat("Target (y) for t =", sample_indices[1], "\n")
print(y_train[1,])
num_samples <- 256
num_returns <- length(returns_vec)
set.seed(1234)
# We need to sample indices `t` where we can form a state AND check the next return.
# So, `t` must be between `window_size` and `num_returns - 1`.
sample_indices <- sample((window_size):(num_returns - 1),
size = num_samples,
replace = TRUE)
# --- Create the input matrix X ---
# Each row of X will be a state vector.
X_train <- t(sapply(sample_indices, make_state))
# --- Create the target matrix y ---
y_train <- matrix(0, nrow = num_samples, ncol = 2)
colnames(y_train) <- c("y_long", "y_flat")
for (i in 1:num_samples) {
t <- sample_indices[i]
# The rule is based on comparing the return at t+1 with the return at t.
ret_t <- returns_vec[t]
ret_t_plus_1 <- returns_vec[t + 1]
if (ret_t_plus_1 > ret_t) {
# If the next return is higher, the 'long' action was "correct".
y_train[i, "y_long"] <- 1
y_train[i, "y_flat"] <- 0
} else {
# Otherwise, the 'flat' action was "correct".
y_train[i, "y_long"] <- 0
y_train[i, "y_flat"] <- 1
}
}
cat("Dimensions of our training data:\n")
cat("X_train (inputs):", dim(X_train), "\n")
cat("y_train (targets):", dim(y_train), "\n\n")
cat("Example of one training pair:\n")
cat("State (X) for t =", sample_indices[1], "\n")
print(X_train[1,])
cat("Target (y) for t =", sample_indices[1], "\n")
print(y_train[1,])
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam(learning_rate = 1e-3)
)
# Train the model
history <- model %>% fit(
x = X_train,
y = y_train,
epochs = 20,
batch_size = 32,
validation_split = 0.2
)
plot(history, main = "Model Loss Over Epochs")
# Get the most recent state vector
t_recent <- num_returns
s_recent <- make_state(t_recent)
# Keras models expect a batch of inputs (a matrix), even if it's just one sample.
# We need to reshape our state vector into a 1-row matrix.
s_recent_reshaped <- array_reshape(s_recent, c(1, window_size))
# Predict the Q-values for the most recent state
predicted_q_values <- model %>% predict(s_recent_reshaped)
cat("Most recent state (last 10 returns):\n")
print(s_recent)
cat("\nPredicted Q-Values for this state:\n")
cat(sprintf("Q(s, 'long'): %.4f\n", predicted_q_values[1, 1]))
cat(sprintf("Q(s, 'flat'): %.4f\n", predicted_q_values[1, 2]))
# Interpret the result
best_action <- if (predicted_q_values[1, 1] > predicted_q_values[1, 2]) "long" else "flat"
cat(sprintf("\nBased on these Q-values, the network's recommended action is: '%s'\n", best_action))
# Get the most recent state vector
t_recent <- num_returns
s_recent <- make_state(t_recent)
# reshape our state vector into a 1-row matrix for keras input shape
s_recent_reshaped <- array_reshape(s_recent, c(1, window_size))
# Predict the Q-values for the most recent state
predicted_q_values <- model %>% predict(s_recent_reshaped)
cat("Most recent state (last 10 returns):\n")
print(s_recent)
cat("\nPredicted Q-Values for this state:\n")
cat(sprintf("Q(s, 'long'): %.4f\n", predicted_q_values[1, 1]))
cat(sprintf("Q(s, 'flat'): %.4f\n", predicted_q_values[1, 2]))
# Interpret the result
best_action <- if (predicted_q_values[1, 1] > predicted_q_values[1, 2]) "long" else "flat"
cat(sprintf("\nBased on these Q-values, the network's recommended action is: '%s'\n", best_action))
tinytex::install_tinytex()
renv::status()
install.packages("igrpah")
install.packages("igraph")
setwd("C:/Users/udwal/Downloads")
setwd("C:/Users/udwal/Downloads")
setwd("C:\Users\\udwal\\Documents\\Studies_Viadrina\\Thesis\\main\\images")
setwd("C:/Users/udwal/Documents/Studies_Viadrina/Thesis/main/images")
